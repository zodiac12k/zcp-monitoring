serverFiles:
  alertmanager.rules:
    groups:
    - name: alertmanager.rules
      rules:
      - alert: AlertmanagerFailedReload
        expr: alertmanager_config_last_reload_successful == 0
        for: 10m
        labels:
          severity: critical
          product: zcp-public
          priority: P4
        annotations:
          description: Reloading Alertmanager's configuration has failed
          summary: Alertmanager configuration reload has failed
  component-up-monitoring.rules:
    groups:
    - name: component-up-monitoring.rules
      rules:
      - alert: MonitoringComponentDown
        expr: up{component=~"alertmanager|blackbox-exporter|grafana|kube-state-metrics|prometheus"} == 0
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P4
        annotations:
          summary: Monitoring Component Down
          description: Monitoring Component has failed for {{ $labels.namespace }}/{{ $labels.component }}
  elasticsearch.rules:
    groups:
    - name: elasticsearch.rules
      rules:
      - record: elasticsearch_filesystem_data_used_percent
        expr: 100 * (elasticsearch_filesystem_data_size_bytes - elasticsearch_filesystem_data_free_bytes) / elasticsearch_filesystem_data_size_bytes
      - record: elasticsearch_filesystem_data_free_percent
        expr: 100 - elasticsearch_filesystem_data_used_percent
      - alert: ElasticsearchClusterDown
        expr: elasticsearch_cluster_health_up == 0
        for: 2m
        labels:
          product: zcp-public
          priority: P4
        annotations:
          summary: "Elasticsearch cluster down"
          description: "Elasticsearch cluster is down"
      - alert: ElasticsearchDataNodeLowDisk
        expr: elasticsearch_filesystem_data_used_percent{es_data_node="true"} > 80
        for: 30m
        labels:
          product: zcp-public
          priority: P3
        annotations:
          summary: "Elasticsearch: Data node low disk space"
          description: "Elasticsearch: Data node {{ $labels.exported_name }} disk usage is above 80% (current value is: {{ $value }})"
      - alert: ElasticsearchClusterNotHealthy
        expr: elasticsearch_cluster_health_status{color="yellow"} == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P4
        annotations:
          summary: "Elasticsearch cluster is not healthy"
          description: "Elasticsearch cluster is not healthy (current color is: {{ $labels.color }})"
      - alert: ElasticsearchClusterNotHealthy
        expr: elasticsearch_cluster_health_status{color="red"} == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P4
        annotations:
          summary: "Elasticsearch cluster is not healthy"
          description: "Elasticsearch cluster is not healthy (current color is: {{ $labels.color }})"
  kube-apiserver.rules:
    groups:
    - name: kube-apiserver.rules
      rules:
      - alert: APIServerDown
        expr: up{job="kubernetes-apiservers"} == 0
        for: 5m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: API server unreachable
          description: Prometheus failed to scrape API server
      - alert: APIServerErrorsHigh
        expr: sum(rate(apiserver_request_count{code=~"^(?:5..)$"}[5m])) / sum(rate(apiserver_request_count[5m])) * 100 > 1
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: API server Errors High
          description: "API server returns errors for {{$value}}% of requests summary: API server request errors"
      - alert: APIServerLatencyHigh
        expr: avg(apiserver_request_latencies_sum / apiserver_request_latencies_count) / 1000000 > 2000
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: API server Latency High
          description: "API server latency High above 2s (current value: {{ $value }}ms"
  kube-state-metrics.rules:
    groups:
    - name: kube-state-metrics.rules
      rules:
      - record: kube_node_status_condition_ready
        expr: kube_node_status_condition{condition="Ready",status="true"}
      - alert: K8SManagementNodeNotReady
        expr: kube_node_labels{dedicated="management"} and on (node) kube_node_status_condition_ready == 0
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}) status is NotReady"
          description: "The kubelet on {{$labels.node}} has not checked in with the API, or has set itself to NotReady, for more than 2m."
      - alert: K8SLoggingNodeNotReady
        expr: kube_node_labels{dedicated="logging"} and on (node) kube_node_status_condition_ready == 0
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}) status is NotReady"
          description: "The kubelet on {{$labels.node}} has not checked in with the API, or has set itself to NotReady, for more than 2m."
      - alert: K8SWorkerNodeNotReady
        expr: kube_node_labels{dedicated=~"worker|ha-worker"} and on (node) kube_node_status_condition_ready == 0
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) status is NotReady"
          description: "The kubelet on {{$labels.node}} has not checked in with the API, or has set itself to NotReady, for more than 2m."
      - alert: K8SEdgeNodeNotReady
        expr: kube_node_labels{dedicated="edge"} and on (node) kube_node_status_condition_ready == 0
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) status is NotReady"
          description: "The kubelet on {{$labels.node}} has not checked in with the API, or has set itself to NotReady, for more than 2m."
      - alert: K8SZDBNodeNotReady
        expr: kube_node_labels{dedicated="zdb"} and on (node) kube_node_status_condition_ready == 0
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) status is NotReady"
          description: "The kubelet on {{$labels.node}} has not checked in with the API, or has set itself to NotReady, for more than 2m."

      - record: kube_node_status_condition_out_of_disk
        expr: kube_node_status_condition{condition="OutOfDisk",status="true"}
      - alert: K8SManagementNodeOutOfDisk
        expr: kube_node_labels{dedicated="management"} and on (node) kube_node_status_condition_out_of_disk == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}) ran out of disk space."
          description: "{{$labels.node}} is insufficient free space on the node for adding new pods."
      - alert: K8SLoggingNodeOutOfDisk
        expr: kube_node_labels{dedicated="logging"} and on (node) kube_node_status_condition_out_of_disk == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}) ran out of disk space."
          description: "{{$labels.node}} is insufficient free space on the node for adding new pods."
      - alert: K8SWorkerNodeOutOfDisk
        expr: kube_node_labels{dedicated=~"worker|ha-worker"} and on (node) kube_node_status_condition_out_of_disk == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) ran out of disk space."
          description: "{{$labels.node}} is insufficient free space on the node for adding new pods."
      - alert: K8SEdgeNodeOutOfDisk
        expr: kube_node_labels{dedicated="edge"} and on (node) kube_node_status_condition_out_of_disk == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) ran out of disk space."
          description: "{{$labels.node}} is insufficient free space on the node for adding new pods."
      - alert: K8SZDBNodeOutOfDisk
        expr: kube_node_labels{dedicated="zdb"} and on (node) kube_node_status_condition_out_of_disk == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) ran out of disk space."
          description: "{{$labels.node}} is insufficient free space on the node for adding new pods."

      - record: kube_node_status_condition_memory_pressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"}
      - alert: K8SManagementNodeMemoryPressure
        expr: kube_node_labels{dedicated="management"} and on (node) kube_node_status_condition_memory_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}) is under memory pressure."
          description: "{{$labels.node}} memory is low."
      - alert: K8SLoggingNodeMemoryPressure
        expr: kube_node_labels{dedicated="logging"} and on (node) kube_node_status_condition_memory_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}) is under memory pressure."
          description: "{{$labels.node}} memory is low."
      - alert: K8SWorkerNodeMemoryPressure
        expr: kube_node_labels{dedicated=~"worker|ha-worker"} and on (node) kube_node_status_condition_memory_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) is under memory pressure."
          description: "{{$labels.node}} memory is low."
      - alert: K8SEdgeNodeMemoryPressure
        expr: kube_node_labels{dedicated="edge"} and on (node) kube_node_status_condition_memory_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) is under memory pressure."
          description: "{{$labels.node}} memory is low."
      - alert: K8SZDBNodeMemoryPressure
        expr: kube_node_labels{dedicated="zdb"} and on (node) kube_node_status_condition_memory_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) is under memory pressure."
          description: "{{$labels.node}} memory is low."

      - record: kube_node_status_condition_pid_pressure
        expr: kube_node_status_condition{condition="PIDPressure",status="true"}
      - alert: K8SManagementNodePIDPressure
        expr: kube_node_labels{dedicated="management"} and on (node) kube_node_status_condition_pid_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}) is under PID pressure."
          description: "{{$labels.node}} is too many processes on the node."
      - alert: K8SLoggingNodePIDPressure
        expr: kube_node_labels{dedicated="logging"} and on (node) kube_node_status_condition_pid_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}) is under PID pressure."
          description: "{{$labels.node}} is too many processes on the node."
      - alert: K8SWorkerNodePIDPressure
        expr: kube_node_labels{dedicated=~"worker|ha-worker"} and on (node) kube_node_status_condition_pid_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) is under PID pressure."
          description: "{{$labels.node}} is too many processes on the node."
      - alert: K8SEdgeNodePIDPressure
        expr: kube_node_labels{dedicated="edge"} and on (node) kube_node_status_condition_pid_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) is under PID pressure."
          description: "{{$labels.node}} is too many processes on the node."
      - alert: K8SZDBNodePIDPressure
        expr: kube_node_labels{dedicated="zdb"} and on (node) kube_node_status_condition_pid_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) is under PID pressure."
          description: "{{$labels.node}} is too many processes on the node."

      - record: kube_node_status_condition_disk_pressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"}
      - alert: K8SManagementNodeDiskPressure
        expr: kube_node_labels{dedicated="management"} and on (node) kube_node_status_condition_disk_pressure == 1
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}) is under disk pressure."
          description: "{{$labels.node}} disk capacity is low."
      - alert: K8SLoggingNodeDiskPressure
        expr: kube_node_labels{dedicated="logging"} and on (node) kube_node_status_condition_disk_pressure == 1
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}) is under disk pressure."
          description: "{{$labels.node}} disk capacity is low."
      - alert: K8SWorkerNodeDiskPressure
        expr: kube_node_labels{dedicated=~"worker|ha-worker"} and on (node) kube_node_status_condition_disk_pressure == 1
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) is under disk pressure."
          description: "{{$labels.node}} disk capacity is low."
      - alert: K8SEdgeNodeDiskPressure
        expr: kube_node_labels{dedicated="edge"} and on (node) kube_node_status_condition_disk_pressure == 1
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) is under disk pressure."
          description: "{{$labels.node}} disk capacity is low."
      - alert: K8SZDBNodeDiskPressure
        expr: kube_node_labels{dedicated="zdb"} and on (node) kube_node_status_condition_disk_pressure == 1
        for: 2m
        labels:
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}) is under disk pressure."
          description: "{{$labels.node}} disk capacity is low."

      - alert: PodFrequentlyRestarting
        expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
        for: 10m
        labels:
          severity: warning
          product: zcp-public
          priority: P5
        annotations:
          summary: "Pod is restarting frequently"
          description: "Pod {{$labels.namespace}}/{{$labels.pod}} is was restarted {{$value}} times within the last hour"
      - alert: PodAbnormallyTerminated
        expr: sum_over_time(kube_pod_container_status_terminated_reason{namespace!~"kube-system|ibm-system|zcp-system|zdb-system|zdb-managed",reason!~"Completed|Error"}[5m]) > 0
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P3
        annotations:
          summary: "Pod is abnormally terminated"
          description: "Pod {{$labels.namespace}}/{{$labels.pod}}/{{$labels.container}} is abnormally terminated for a {{$labels.reason}}"
      - alert: PodAbnormallyTerminated
        expr: sum_over_time(kube_pod_container_status_terminated_reason{namespace=~"kube-system|ibm-system|zcp-system|zdb-system|zdb-managed",reason!~"Completed|Error"}[5m]) > 0
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Pod is abnormally terminated"
          description: "Pod {{$labels.namespace}}/{{$labels.pod}}/{{$labels.container}} is abnormally terminated for a {{$labels.reason}}"
      - alert: PodAbnormallyWaiting
        expr: sum_over_time(kube_pod_container_status_waiting_reason{namespace!~"kube-system|ibm-system|zcp-system|zdb-system|zdb-managed",reason!="ContainerCreating"}[5m]) > 0
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P3
        annotations:
          summary: "Pod is abnormally waiting"
          description: "Pod {{$labels.namespace}}/{{$labels.pod}}/{{$labels.container}} is abnormally waiting for a {{$labels.reason}}"
      - alert: PodAbnormallyWaiting
        expr: sum_over_time(kube_pod_container_status_waiting_reason{namespace=~"kube-system|ibm-system|zcp-system|zdb-system|zdb-managed",reason!="ContainerCreating"}[5m]) > 0
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Pod is abnormally waiting"
          description: "Pod {{$labels.namespace}}/{{$labels.pod}}/{{$labels.container}} is abnormally waiting for a {{$labels.reason}}"
      - alert: PodErrorTerminated
        expr: sum_over_time(kube_pod_container_status_terminated_reason{reason="Error"}[5m]) > 0
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P5
        annotations:
          summary: "Pod is terminated for an error"
          description: "Pod {{$labels.namespace}}/{{$labels.pod}}/{{$labels.container}} is abnormally terminated for an Error"
  kubelet.rules:
    groups:
    - name: kubelet.rules
      rules:
      - alert: K8SKubeletDown
        expr: absent(up{job="kubernetes-nodes"} == 1)
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P4
        annotations:
          description: "Prometheus failed to scrape {{ $labels.instance }} of kubelet."
          summary: "Kubelet cannot be scraped"
      - alert: K8SKubeletTooManyPods
        expr: kubelet_running_pod_count > 100
        for: 10m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          description: "Kubelet {{$labels.instance}} is running {{$value}} pods, close to the limit of 110"
          summary: "Kubelet is close to pod limit"
      - alert: PersistentVolumeLowSpace
        expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100 > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P3
        annotations:
          summary: "Persistent Volume({{$labels.namespace}}/{{$labels.persistentvolumeclaim}}): Low disk space"
          description: "Persistent Volume({{$labels.namespace}}/{{$labels.persistentvolumeclaim}}): Disk usage is above 80% (current value is: {{$value}})"
      - alert: PersistentVolumeLowSpace
        expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes * 100 > 95
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P1
        annotations:
          summary: "Persistent Volume({{$labels.namespace}}/{{$labels.persistentvolumeclaim}}): Low disk space"
          description: "Persistent Volume({{$labels.namespace}}/{{$labels.persistentvolumeclaim}}): Disk usage is above 80% (current value is: {{$value}})"
  node.rules:
    groups:
    - name: node.rules
      rules:
      - alert: NodeExporterDown
        expr: absent(up{component="node-exporter"} == 1)
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P4
        annotations:
          summary: node-exporter cannot be scraped
          description: Prometheus failed to scrape Node({{$labels.instance}}) for more than 2m

      - record: node_cpu_usage_rate
        expr: (100 - (avg by(dedicated,instance)(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100))
      - alert: ManagementNodeCPUUsage
        expr: node_cpu_usage_rate{dedicated="management"} > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 80% (current value is: {{$value}})"
      - alert: LoggingNodeCPUUsage
        expr: node_cpu_usage_rate{dedicated="logging"} > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 80% (current value is: {{$value}})"
      - alert: WorkerNodeCPUUsage
        expr: node_cpu_usage_rate{dedicated=~"worker|ha-worker"} > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P3
        annotations:
          summary: "Node({{$labels.dedicated}}): High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 80% (current value is: {{$value}})"
      - alert: EdgeNodeCPUUsage
        expr: node_cpu_usage_rate{dedicated="edge"} > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P3
        annotations:
          summary: "Node({{$labels.dedicated}}): High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 80% (current value is: {{$value}})"
      - alert: ZDBNodeCPUUsage
        expr: node_cpu_usage_rate{dedicated="zdb"} > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P3
        annotations:
          summary: "Node({{$labels.dedicated}}): High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 80% (current value is: {{$value}})"

      - alert: ManagementNodeCPUUsage
        expr: node_cpu_usage_rate{dedicated="management"} > 95
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}): High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 95% (current value is: {{$value}})"
      - alert: LoggingNodeCPUUsage
        expr: node_cpu_usage_rate{dedicated="logging"} > 95
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P2
        annotations:
          summary: "Node({{$labels.dedicated}}): High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 95% (current value is: {{$value}})"
      - alert: WorkerNodeCPUUsage
        expr: node_cpu_usage_rate{dedicated=~"worker|ha-worker"} > 95
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 95% (current value is: {{$value}})"
      - alert: EdgeNodeCPUUsage
        expr: node_cpu_usage_rate{dedicated="edge"} > 95
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 95% (current value is: {{$value}})"
      - alert: ZDBNodeCPUUsage
        expr: node_cpu_usage_rate{dedicated="zdb"} > 95
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 95% (current value is: {{$value}})"

      - alert: ManagementNodeLoadAverage5
        expr: node_load5{dedicated="management"} / count without (cpu, mode) (node_cpu_seconds_total{mode="system"}) > 1
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): High Load Average detected"
          description: "{{$labels.instance}}: Load Average is high (current value is: {{$value}})"
      - alert: LoggingNodeLoadAverage5
        expr: node_load5{dedicated="logging"} / count without (cpu, mode) (node_cpu_seconds_total{mode="system"}) > 1
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): High Load Average detected"
          description: "{{$labels.instance}}: Load Average is high (current value is: {{$value}})"
      - alert: WorkerNodeLoadAverage5
        expr: node_load5{dedicated=~"worker|ha-worker"} / count without (cpu, mode) (node_cpu_seconds_total{mode="system"}) > 1
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): High Load Average detected"
          description: "{{$labels.instance}}: Load Average is high (current value is: {{$value}})"
      - alert: EdgeNodeLoadAverage5
        expr: node_load5{dedicated="edge"} / count without (cpu, mode) (node_cpu_seconds_total{mode="system"}) > 1
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): High Load Average detected"
          description: "{{$labels.instance}}: Load Average is high (current value is: {{$value}})"
      - alert: ZDBNodeLoadAverage5
        expr: node_load5{dedicated="zdb"} / count without (cpu, mode) (node_cpu_seconds_total{mode="system"}) > 1
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): High Load Average detected"
          description: "{{$labels.instance}}: Load Average is high (current value is: {{$value}})"

      - record: node_memory_MemUsed_bytes
        expr: node_memory_MemTotal_bytes - node_memory_MemFree_bytes - node_memory_Buffers_bytes - node_memory_Cached_bytes
      - record: node_memory_MemUsed_percent
        expr: node_memory_MemUsed_bytes / node_memory_MemTotal_bytes * 100
      - record: node_memory_MemUnavailable_bytes
        expr: node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes
      - record: node_memory_MemUnavailable_percent
        expr: node_memory_MemUnavailable_bytes / node_memory_MemTotal_bytes * 100
      - alert: ManagementNodeMemoryUsage
        expr: node_memory_MemUsed_percent{dedicated="management"} > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 80% (current value is: {{$value}})"
      - alert: LoggingNodeMemoryUsage
        expr: node_memory_MemUsed_percent{dedicated="logging"} > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 80% (current value is: {{$value}})"
      - alert: WorkerNodeMemoryUsage
        expr: node_memory_MemUsed_percent{dedicated=~"worker|ha-worker"} > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P3
        annotations:
          summary: "Node({{$labels.dedicated}}): High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 80% (current value is: {{$value}})"
      - alert: EdgeNodeMemoryUsage
        expr: node_memory_MemUsed_percent{dedicated="edge"} > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P3
        annotations:
          summary: "Node({{$labels.dedicated}}): High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 80% (current value is: {{$value}})"
      - alert: ZDBNodeMemoryUsage
        expr: node_memory_MemUsed_percent{dedicated="zdb"} > 80
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P3
        annotations:
          summary: "Node({{$labels.dedicated}}): High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 80% (current value is: {{$value}})"

      - alert: ManagementNodeMemoryUsage
        expr: node_memory_MemUsed_percent{dedicated="management"} > 95
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 95% (current value is: {{$value}})"
      - alert: LoggingNodeMemoryUsage
        expr: node_memory_MemUsed_percent{dedicated="logging"} > 95
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 95% (current value is: {{$value}})"
      - alert: WorkerNodeMemoryUsage
        expr: node_memory_MemUsed_percent{dedicated=~"worker|ha-worker"} > 95
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 95% (current value is: {{$value}})"
      - alert: EdgeNodeMemoryUsage
        expr: node_memory_MemUsed_percent{dedicated="edge"} > 95
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 75% (current value is: {{$value}})"
      - alert: ZDBNodeMemoryUsage
        expr: node_memory_MemUsed_percent{dedicated="zdb"} > 95
        for: 2m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 95% (current value is: {{$value}})"

      - alert: NodeSwapUsage
        expr: ((node_memory_SwapTotal_bytes - node_memory_SwapFree_bytes) / node_memory_SwapTotal_bytes) * 100 > 75
        for: 2m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "{{$labels.instance}}: Swap usage detected"
          description: "{{$labels.instance}}: Swap usage usage is above 75% (current value is: {{ $value }})"

      - record: node_filesystem_used_bytes
        expr: node_filesystem_size_bytes - node_filesystem_avail_bytes
      - record: node_filesystem_used_percent
        expr: node_filesystem_used_bytes / node_filesystem_size_bytes * 100
      - alert: ManagementNodeLowRootDisk
        expr: node_filesystem_used_percent{dedicated="management",mountpoint="/"} > 80
        for: 30m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): Low root disk space"
          description: "{{$labels.instance}}: Root disk usage is above 80% (current value is: {{$value}})"
      - alert: LoggingNodeLowRootDisk
        expr: node_filesystem_used_percent{dedicated="logging",mountpoint="/"} > 80
        for: 30m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): Low root disk space"
          description: "{{$labels.instance}}: Root disk usage is above 80% (current value is: {{$value}})"
      - alert: WorkerNodeLowRootDisk
        expr: node_filesystem_used_percent{dedicated=~"worker|ha-worker",mountpoint="/"} > 80
        for: 30m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): Low root disk space"
          description: "{{$labels.instance}}: Root disk usage is above 80% (current value is: {{$value}})"
      - alert: EdgeNodeLowRootDisk
        expr: node_filesystem_used_percent{dedicated="edge",mountpoint="/"} > 80
        for: 30m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): Low root disk space"
          description: "{{$labels.instance}}: Root disk usage is above 80% (current value is: {{$value}})"
      - alert: ZDBNodeLowRootDisk
        expr: node_filesystem_used_percent{dedicated="zdb",mountpoint="/"} > 80
        for: 30m
        labels:
          severity: warning
          product: zcp-public
          priority: P4
        annotations:
          summary: "Node({{$labels.dedicated}}): Low root disk space"
          description: "{{$labels.instance}}: Root disk usage is above 80% (current value is: {{$value}})"

      - alert: ManagementNodeLowRootDisk
        expr: node_filesystem_used_percent{dedicated="management",mountpoint="/"} > 95
        for: 30m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): Low root disk space"
          description: "{{$labels.instance}}: Root disk usage is above 95% (current value is: {{$value}})"
      - alert: LoggingNodeLowRootDisk
        expr: node_filesystem_used_percent{dedicated="logging",mountpoint="/"} > 95
        for: 30m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): Low root disk space"
          description: "{{$labels.instance}}: Root disk usage is above 95% (current value is: {{$value}})"
      - alert: WorkerNodeLowRootDisk
        expr: node_filesystem_used_percent{dedicated=~"worker|ha-worker",mountpoint="/"} > 95
        for: 30m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): Low root disk space"
          description: "{{$labels.instance}}: Root disk usage is above 95% (current value is: {{$value}})"
      - alert: EdgeNodeLowRootDisk
        expr: node_filesystem_used_percent{dedicated="edge",mountpoint="/"} > 95
        for: 30m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): Low root disk space"
          description: "{{$labels.instance}}: Root disk usage is above 95% (current value is: {{$value}})"
      - alert: ZDBNodeLowRootDisk
        expr: node_filesystem_used_percent{dedicated="zdb",mountpoint="/"} > 95
        for: 30m
        labels:
          severity: critical
          product: zcp-public
          priority: P1
        annotations:
          summary: "Node({{$labels.dedicated}}): Low root disk space"
          description: "{{$labels.instance}}: Root disk usage is above 95% (current value is: {{$value}})"
  prometheus.rules:
    groups:
    - name: prometheus.rules
      rules:
      - alert: PrometheusFailedReload
        expr: prometheus_config_last_reload_successful == 0
        for: 10m
        labels:
          severity: critical
          product: zcp-public
          priority: P4
        annotations:
          summary: Prometheus configuration reload has failed
          description: Reloading Prometheus' configuration has failed for {{ $labels.namespace}}/{{ $labels.pod}}.
      - alert: PrometheusErrorSendingAlerts
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m]) > 0.01
        for: 10m
        labels:
          severity: critical
          product: zcp-public
          priority: P4
        annotations:
          summary: Errors while sending alerts from Prometheus
          description: Errors while sending alerts from Prometheus to Alertmanager {{$labels.Alertmanager}}

  prometheus.yml:
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    ## Below two files are DEPRECATED will be removed from this default values file
    - /etc/config/rules
    - /etc/config/alerts
    - /etc/config/alertmanager.rules
    - /etc/config/component-up-monitoring.rules
    - /etc/config/elasticsearch.rules
    - /etc/config/kube-apiserver.rules
    - /etc/config/kube-state-metrics.rules
    - /etc/config/kubelet.rules
    - /etc/config/node.rules
    - /etc/config/prometheus.rules
#    - /etc/prometheus-rules/*.rules