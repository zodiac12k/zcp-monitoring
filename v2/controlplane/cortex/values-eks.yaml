## Container image settings.
## Since the image is unique for all microservices, so are image settings.

image:
  repository: quay.io/cortexproject/cortex
  tag: v1.1.0
  pullPolicy: IfNotPresent

  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  # pullSecrets:
  #   - myRegistrKeySecretName

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  hosts:
  - host: cortex.mcm-dev.cloudzcp.com
    paths:
    - /
  tls:
  - secretName: cloudzcp-com-cert
    hosts:
    - cortex.mcm-dev.cloudzcp.com

serviceAccount:
  create: true
  name:
  annotations: {}


config:
  auth_enabled: true
  ingester:
    max_transfer_retries: 0
    lifecycler:
      join_after: 0s
      final_sleep: 0s
      num_tokens: 512
      ring:
        replication_factor: 1
        kvstore:
          store: consul
          prefix: "collectors/"
          consul:
            host: "consul-server:8500"
            http_client_timeout: "20s"
            consistent_reads: true
  limits:
    enforce_metric_name: false
    reject_old_samples: true
    reject_old_samples_max_age: 168h
    # Maximum number of label names per series.
    # CLI flag: -validation.max-label-names-per-series
    max_label_names_per_series: 100
  #  schema:
  #    configs:
  #    - from: 2019-07-29
  #      store: s3
  #      object_store: s3
  #      schema: v10
  #      index:
  #        prefix: index_
  #        period: 168h
  server:
    http_listen_port: 8080
    grpc_listen_port: 9095
    grpc_server_max_recv_msg_size: 104857600
    grpc_server_max_send_msg_size: 104857600
    grpc_server_max_concurrent_streams: 1000
    log_level: "info"
  ingester_client:
    grpc_client_config:
      max_recv_msg_size: 104857600
      max_send_msg_size: 104857600
      use_gzip_compression: true
  # See https://github.com/cortexproject/cortex/blob/master/docs/configuration/config-file-reference.md#storage_config
  storage:
    engine: "tsdb"
    cassandra:
      addresses: # configure cassandra addresses here.
      keyspace: cortex # configure desired keyspace here.
      auth: true
      username: # configure cassandra user here.
      password: # configure cassandra password here.
    #    azure:
    #      container_name: # configure azure blob container name here.
    #      account_name: # configure azure storage account name here.
    #      account_key: # configure azure storage account key here.
    #     aws:
    #   dynamodb:
    #     dynamodb_url:
    #     api_limit:
    #     throttle_limit:
    #     metrics:
    #       url:
    #       target_queue_length:
    #       scale_up_factor:
    #       ignore_throttle_below:
    #       queue_length_query:
    #       write_throttle_query:
    #       write_usage_query:
    #       read_usage_query:
    #       read_error_query:
    #     chunk_gang_size:
    #     chunk_get_max_parallelism:
  #       s3:
  #       bucketnames: cloudzcp-eks-dev-cortex-tsdb
  #       s3forcepathstyle:
  #    index_queries_cache_config:
  #      memcached:
  #        expiration: 1h
  #      memcached_client:
  #        timeout: 1s
#  chunk_store:
#    max_look_back_period: 0s
#    chunk_cache_config:
#      memcached:
#        expiration: 1h
#      memcached_client:
#        timeout: 1s
  table_manager:
    retention_deletes_enabled: false
    retention_period: 0s
  distributor:
    shard_by_all_labels: true
    pool:
      health_check_ingesters: true
  querier:
    active_query_tracker_dir: /data/cortex/querier
    query_ingesters_within: 12h
  query_range:
    split_queries_by_interval: 24h
    align_queries_with_step: true
    cache_results: true
    results_cache:
      max_freshness: 1m
      cache:
        memcached:
          expiration: 1h
        memcached_client:
          timeout: 1s
  ruler:
    enable_api: true
    rule_path: "/data/rules"
  alertmanager:
    external_url: "/api/prom/alertmanager"
  frontend:
    #max_outstanding_per_tenant: 1000
    log_queries_longer_than: 10s
    compress_responses: true
  tsdb:
    # Local directory to store TSDBs in the ingesters.
    # CLI flag: -experimental.tsdb.dir
    dir: "/data/tsdb"

    # TSDB blocks retention in the ingester before a block is removed. This should
    # be larger than the block_ranges_period and large enough to give queriers
    # enough time to discover newly uploaded blocks.
    # CLI flag: -experimental.tsdb.retention-period
    retention_period: 6h

    # Backend storage to use. Supported backends are: s3, gcs, azure, filesystem.
    # CLI flag: -experimental.tsdb.backend
    backend: "s3"

    bucket_store:
      # Directory to store synchronized TSDB index headers.
      # CLI flag: -experimental.tsdb.bucket-store.sync-dir
      sync_dir: "/data/tsdb-sync"

      index_cache:
        # The index cache backend type. Supported values: inmemory, memcached.
        # CLI flag: -experimental.tsdb.bucket-store.index-cache.backend
        backend: "memcached"

        memcached:
          # Comma separated list of memcached addresses. Supported prefixes are:
          # dns+ (looked up as an A/AAAA query), dnssrv+ (looked up as a SRV
          # query, dnssrvnoa+ (looked up as a SRV query, with no A/AAAA lookup
          # made after that).
          # CLI flag: -experimental.tsdb.bucket-store.index-cache.memcached.addresses
          addresses: "cortex-memcached-index:11211"

    #      chunks_cache:
    #        # Backend for chunks cache, if not empty. Supported values: memcached.
    #        # CLI flag: -experimental.tsdb.bucket-store.chunks-cache.backend
    #        backend: "memcached"
    #
    #        memcached:
    #          # Comma separated list of memcached addresses. Supported prefixes are:
    #          # dns+ (looked up as an A/AAAA query), dnssrv+ (looked up as a SRV
    #          # query, dnssrvnoa+ (looked up as a SRV query, with no A/AAAA lookup
    #          # made after that).
    #          # CLI flag: -experimental.tsdb.bucket-store.chunks-cache.memcached.addresses
    #          addresses: "cortex-memcached:11211"

    s3:
      # S3 endpoint without schema
      # CLI flag: -experimental.tsdb.s3.endpoint
      endpoint: ""

      # S3 bucket name
      # CLI flag: -experimental.tsdb.s3.bucket-name
      bucket_name: ""

      # S3 secret access key
      # CLI flag: -experimental.tsdb.s3.secret-access-key
      secret_access_key: ""

      # S3 access key ID
      # CLI flag: -experimental.tsdb.s3.access-key-id
      access_key_id: ""
#  purger:
#    # Enable purger to allow deletion of series. Be aware that Delete series feature
#    # is still experimental
#    # CLI flag: -purger.enable
#    enable: true
#
#    # Name of the object store to use for storing delete plans
#    # CLI flag: -purger.object-store-type
#    object_store_type: "aws"

rbac:
  create: true
  pspEnabled: true

alertmanager:
  replicas: 1

  service:
    port: 80
    annotations: {}
    labels: {}

  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 32Mi

  ## Additional Loki container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "http-metrics"

  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - "amd64"
          - key: node-role.kubernetes.io/monitoring
            operator: Exists
  annotations: {}
  persistence:
    subPath:

  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45

  securityContext: {}
    #fsGroup: 10001
    #runAsGroup: 10001
    #runAsNonRoot: true
  #runAsUser: 10001

  ## Tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  # The values to set in the PodDisruptionBudget spec
  # If not set then a PodDisruptionBudget will not be created
  podDisruptionBudget: {}
  # minAvailable: 1
  # maxUnavailable: 1

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 60

  initContainers: []
  ## Init containers to be added to the cortex pod.
  # - name: my-init-container
  #   image: busybox:latest
  #   command: ['sh', '-c', 'echo hello']

  extraContainers: []
  ## Additional containers to be added to the cortex pod.
  # - name: reverse-proxy
  #   image: angelbarrera92/basic-auth-reverse-proxy:dev
  #   args:
  #     - "serve"
  #     - "--upstream=http://localhost:3100"
  #     - "--auth-config=/etc/reverse-proxy-conf/authn.yaml"
  #   ports:
  #     - name: http
  #       containerPort: 11811
  #       protocol: TCP
  #   volumeMounts:
  #     - name: reverse-proxy-auth-config
  #       mountPath: /etc/reverse-proxy-conf


  extraVolumes: []
  ## Additional volumes to the cortex pod.
  # - name: reverse-proxy-auth-config
  #   secret:
  #     secretName: reverse-proxy-auth-config

  ## Extra volume mounts that will be added to the cortex container
  extraVolumeMounts: []

  extraPorts: []
  ## Additional ports to the cortex services. Useful to expose extra container ports.
  # - port: 11811
  #   protocol: TCP
  #   name: http
  #   targetPort: http

  # Extra env variables to pass to the cortex container
  env: []

distributor:
  replicas: 2

  service:
    port: 80
    annotations: {}
    labels: {}

  resources:
    #    limits:
    #      cpu: 2
    #      memory: 4Gi
    requests:
      cpu: 100m
      memory: 512Mi

  ## Additional Loki container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    log.level: info

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "http-metrics"

  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - "amd64"
          - key: node-role.kubernetes.io/monitoring
            operator: Exists
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: target
              operator: In
              values:
              - distributor
          topologyKey: "kubernetes.io/hostname"

  annotations: {}
  persistence:
    subPath:

  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 60

  tolerations: []
  podDisruptionBudget: {}
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

ingester:
  replicas: 3

  service:
    port: 80
    annotations: {}
    labels: {}

  resources:
    requests:
      cpu: 100m
      memory: 512Mi

  ## Additional Loki container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    log.level: info

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "http-metrics"

  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - "amd64"
          - key: node-role.kubernetes.io/monitoring
            operator: Exists
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: target
              operator: In
              values:
              - ingester
          topologyKey: "kubernetes.io/hostname"

  annotations: {}
  persistence:
    subPath:

  livenessProbe:
    failureThreshold: 20 # 10 minutes failure threshold
    httpGet:
      path: /ready
      port: http-metrics
      scheme: HTTP
    initialDelaySeconds: 180
    periodSeconds: 30
    successThreshold: 1
    timeoutSeconds: 1
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 60

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 240

  tolerations: []
  podDisruptionBudget: {}
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

ruler:
  replicas: 1

  service:
    port: 80
    annotations: {}
    labels: {}

  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 32Mi

  ## Additional Loki container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "http-metrics"

  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - "amd64"
          - key: node-role.kubernetes.io/monitoring
            operator: Exists
  annotations: {}
  persistence:
    subPath:

  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 180

  tolerations: []
  podDisruptionBudget: {}
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

querier:
  replicas: 2

  service:
    port: 80
    annotations: {}
    labels: {}

  resources:
    #    limits:
    #      cpu: 1
    #      memory: 1Gi
    requests:
      cpu: 50m
      memory: 128Mi

  ## Additional Loki container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "http-metrics"

  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - "amd64"
          - key: node-role.kubernetes.io/monitoring
            operator: Exists
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: target
              operator: In
              values:
              - querier
          topologyKey: "kubernetes.io/hostname"

  annotations: {}
  persistence:
    subPath:

  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 180

  tolerations: []
  podDisruptionBudget: {}
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

query_frontend:
  replicas: 2

  service:
    http_port: 80
    grpc_port: 9095
    annotations: {}
    labels: {}

  resources:
    limits:
      cpu: 1
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 32Mi

  ## Additional Loki container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "http-metrics"

  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - "amd64"
          - key: node-role.kubernetes.io/monitoring
            operator: Exists
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: target
              operator: In
              values:
              - query-frontend
          topologyKey: "kubernetes.io/hostname"

  annotations: {}
  persistence:
    subPath:

  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 180

  tolerations: []
  podDisruptionBudget: {}
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

table_manager:
  replicas: 1

  service:
    port: 80
    annotations: {}
    labels: {}

  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 10m
      memory: 32Mi

  ## Additional Loki container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "http-metrics"

  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - "amd64"
          - key: node-role.kubernetes.io/monitoring
            operator: Exists
  annotations: {}
  persistence:
    subPath:

  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 180

  tolerations: []
  podDisruptionBudget: {}
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

configs:
  replicas: 1

  service:
    port: 80
    annotations: {}
    labels: {}

  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 10m
      memory: 32Mi

  ## Additional Loki container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "http-metrics"

  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - "amd64"
          - key: node-role.kubernetes.io/monitoring
            operator: Exists
  annotations: {}
  persistence:
    subPath:

  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 180

  tolerations: []
  podDisruptionBudget: {}
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

nginx:
  replicas: 2
  http_listen_port: 80
  image:
    repository: nginx
    tag: 1.17
    pullPolicy: IfNotPresent

  service:
    port: 80
    annotations: {}
    labels: {}

  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 10m
      memory: 16Mi

  ## Additional Loki container arguments, e.g. log level (debug, info, warn, error)
  extraArgs: {}
  #    log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: ""
    prometheus.io/port: "http-metrics"

  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - "amd64"
          - key: node-role.kubernetes.io/monitoring
            operator: Exists
  annotations: {}
  persistence:
    subPath:

  livenessProbe:
    httpGet:
      path: /healthz
      port: http-metrics
    initialDelaySeconds: 10
  readinessProbe:
    httpGet:
      path: /healthz
      port: http-metrics
    initialDelaySeconds: 10

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 10

  tolerations: []
  podDisruptionBudget: {}
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

# chunk caching
memcached:
  enabled: false
  replicaCount: 2
  pdbMinAvailable: 1
  memcached:
    maxItemMemory: 3840
    extraArgs:
    - -I 32m
    threads: 32
  resources:
    requests:
      memory: 1Gi
      cpu: 10m
    limits:
      memory: 4Gi
      cpu: 1
  metrics:
    enabled: true
  #tolerations: {}
  # - key: "dedicated"
  #   operator: "Equal"
  #   value: "cortex-memcached"
  #   effect: "NoSchedule"
  #affinity: {}
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #         nodeSelectorTerms:
  #         - matchExpressions:
  #           - key: dedicated
  #             operator: In
  #             values:
  #             - cortex-memcached

# index caching
memcached-index:
  enabled: true
  replicaCount: 2
  pdbMinAvailable: 1
  memcached:
    maxItemMemory: 3840
    extraArgs:
    - -I 32m
    threads: 32
  resources:
    requests:
      memory: 1Gi
      cpu: 10m
    limits:
      memory: 4Gi
      cpu: 1
  metrics:
    enabled: true
  #tolerations: {}
  # - key: "dedicated"
  #   operator: "Equal"
  #   value: "cortex-memcached"
  #   effect: "NoSchedule"
  #affinity: {}
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #         nodeSelectorTerms:
  #         - matchExpressions:
  #           - key: dedicated
  #             operator: In
  #             values:
  #             - cortex-memcached

# index read caching
memcached-index-read:
  enabled: false
  replicaCount: 2
  #pdbMinAvailable: 1
  #image: memcached:1.5.7-alpine
  memcached:
    maxItemMemory: 3840
    extraArgs:
    - -I 32m
    threads: 32
  resources:
    requests:
      memory: 1Gi
      cpu: 10m
    limits:
      memory: 4Gi
      cpu: 1
  metrics:
    enabled: true
  #tolerations: []
  # - key: "dedicated"
  #   operator: "Equal"
  #   value: "cortex-memcached"
  #   effect: "NoSchedule"
  #affinity: {}
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #         nodeSelectorTerms:
  #         - matchExpressions:
  #           - key: dedicated
  #             operator: In
  #             values:
  #             - cortex-memcached

# index write caching
memcached-index-write:
  enabled: false
  replicaCount: 2
  #dpdbMinAvailable: 1
  #image: memcached:1.5.7-alpine
  memcached:
    maxItemMemory: 3840
    extraArgs:
    - -I 32m
    threads: 32
  resources:
    requests:
      memory: 1Gi
      cpu: 10m
    limits:
      memory: 4Gi
      cpu: 1
  metrics:
    enabled: true
  #tolerations: []
  # - key: "dedicated"
  #   operator: "Equal"
  #   value: "cortex-memcached"
  #   effect: "NoSchedule"
  # affinity: {}
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #         nodeSelectorTerms:
  #         - matchExpressions:
  #           - key: dedicated
  #             operator: In
  #             values:
  #             - cortex-memcached
configsdb_postgresql:
  enabled: true
  uri: postgres://postgres@cortex-configs-db-postgresql:5432/configs?sslmode=disable
  auth:
    password:
    existing_secret:
      name: cortex-configs-db-postgresql
      key: postgresql-password

configsdb-postgresql:
  enabled: true
  image:
    registry: docker.io
    repository: bitnami/postgresql
    tag: 11.7.0-debian-10-r9
  existingSecret: cortex-configsdb-postgresql
  postgresqlDatabase: configs
  serviceAccount:
    enabled: true
  persistence:
    enabled: true
    size: 16Gi
    storageClass: ebs-gp2
  resources:
    requests:
      memory: 64Mi
      cpu: 10m
    limits:
      memory: 512Mi
      cpu: 200mG
